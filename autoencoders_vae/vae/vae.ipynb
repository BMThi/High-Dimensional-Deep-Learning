{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "Learning Outcomes:\n",
    "- Train convolutional VAEs using the reparametrization trick.\n",
    "- Generete new, unseen data by sampling from the latent space.\n",
    "- Illustrate interpolation between different images thanks to latent representations.\n",
    "- Visualize the effect of different weights on the regularization term on the learnt latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "The following coad loads the MNIST dataset and builds the necessary dataloaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "batch_size = 128\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the VAE class\n",
    "In this section we will define the VAE class that we will train and use for image generation. We make the choice of training a Convolutional VAE, with the following architecture (once more, we leave the number of hidden dimensions $p$ as a free parameter):\n",
    "- **The Encoder:** The encoder will consist of the following layers:\n",
    "    - Convolution layer with 32 filters, a kernel size of 4, stride 2 and padding 1.\n",
    "    - BatchNorm layer keeping the same number of features\n",
    "    - ReLu activation\n",
    "    - Convolution layer with 64 filters, a kernel size of 4, stride 2 and padding 1.\n",
    "    - BatchNorm layer keeping the same number of features\n",
    "    - ReLu activation\n",
    "    - Convolution layer with 128 filters, a kernel size of 3, stride 2 and padding 1.\n",
    "    - Batchnorm layer keeping the same number of features\n",
    "    - ReLU layer\n",
    "\n",
    "- **The Latent Space:** The encoder outputs are converted into the mean vector $\\mu$ and logarithm of the variance vector $\\log\\sigma^2$, via two paraller fully connected layers. We will need to define:\n",
    "    - A FC layer to map the output of the encoder $E(x)$ to the mean vector $\\mu(x)$.\n",
    "    - A FC layer to map the output of the encoder $E(x)$ to the log-variance vector $\\log\\sigma^2(x)$.\n",
    "    - A FC layer to map the sampled hidden stacte $z(x)\\sim\\mathcal{N}(\\mu(x),Diag(\\sigma(x)))$ to the decoder input.\n",
    "\n",
    "- **The Decoder.** The decoder will consist of the following layers:\n",
    "    - Deconvolution layer with 64 filters, a kernel size of 3, stride 2 and padding 1.\n",
    "    - BatchNorm layer keeping the same number of features\n",
    "    - ReLu activation\n",
    "    - Deconvolution layer with 32 filters, a kernel size of 4, stride 2 and padding 1.\n",
    "    - BatchNorm layer keeping the same number of features\n",
    "    - ReLu activation\n",
    "    - Deconvolution layer with 1 filter, a kernel size of 4, stride 2 and padding 1.\n",
    "    - Sigmoid layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=10):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # Output: (32, 14, 14)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # TODO: Add the missing encoder layers\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for mean and log variance\n",
    "        self.fc_mu = # TODO: Add the missing FC layer\n",
    "        self.fc_logvar = # TODO: Add the missing FC layer\n",
    "        self.fc_decode = # TODO: Add the missing FC layer\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1),  # Output: (64, 8, 8)\n",
    "            # TODO: Add the missing decoder layers\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = # TODO: Encode the input image\n",
    "        x = # Flatten the output of the convolutional layers\n",
    "        mu = # TODO: Apply the corresponding FC layer\n",
    "        logvar = # TODO: Apply the corresponding FC layer\n",
    "        return mu, logvar\n",
    "    \n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        # TODO: Sample z from the Gaussian with the given mu and logvar\n",
    "        # by using the reparameterization trick: z = mu + sigma * epsilon\n",
    "    \n",
    "    # TODO: Implement the decode function\n",
    "\n",
    "\n",
    "    # TODO: Implement the forward function\n",
    "    # by combining the encode, sample and decode functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/conv_vae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/vae_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the VAE\n",
    "In order to be able to visualize the latent space of the VAE, we will choose a latent dimension equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 2\n",
    "learning_rate = 1e-3\n",
    "epochs = 30\n",
    "beta = 1\n",
    "\n",
    "# TODO: Initialize the VAE model and the Adam optimizer\n",
    "# and move the model to the device\n",
    "\n",
    "# TODO: Train the model for the given number of epochs\n",
    "# At the end of each epoch, print the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/train_vae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the results\n",
    "We first check if the vae model has learnt meaningful features, by plotting a bunch of images from the test set along with their respective reconstructions.\n",
    "\n",
    "We have already define a function called `image_comparison` in the previous noteboo, that does exactly what we want. We can either copy paste it below, or better yet, create a file in the current folder called `utils.py`, copy-paste the function there, along with all the necessary library imports, and then import the `image_comparison` function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define or import the image_comparison function\n",
    "\n",
    "# Select a batch of images from the test dataset\n",
    "random_images = next(iter(test_loader))\n",
    "\n",
    "# Get the reconstructions of the selected images\n",
    "recons, _, _ = vae(random_images[0].to(device))\n",
    "\n",
    "# Reshape the images for plotting\n",
    "random_images = random_images[0].cpu().numpy().squeeze()\n",
    "recons = recons.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the original images and their reconstructions\n",
    "image_comparison(random_images, recons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image generation\n",
    "The puropose of this section is to generate new images that look like MNIST digits. In order to do so, we follow the steps below:\n",
    "- Sample $z$ from a $\\mathcal{N}(0, I)$ distribution ($I$ being the identity matrix of size $p$).\n",
    "- Decode $z$ using the decoder of the VAE to generate a new image.\n",
    "\n",
    "**Question.** Why are we sampling $z$ from a $\\mathcal{N}(0,I)$ distribution? What happened to the learnt mean and variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_samples=10):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        # TODO: Sample random latent vectors\n",
    "        samples = # TODO: Decode the latent vectors\n",
    "        samples = samples.cpu().view(num_samples, 1, 28, 28) # Reshape the samples\n",
    "\n",
    "        fig, ax = plt.subplots(1, num_samples, figsize=(15, 2))\n",
    "        for i in range(num_samples):\n",
    "            ax[i].imshow(samples[i].squeeze(0), cmap='gray')\n",
    "            ax[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "generate_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sample_gen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** As a follow-up exercise, you can check how if the quality of the generated samples improves when using a VAE trained with a larger hidden dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpolation between Images\n",
    "The objective of this section is to visualize the difference between the space of latent representations and the (original) pixel space. In order to do so, we will perform *image interpolation*, i.e., we will take two random images $x_2$ and $x_2$ from the test set, and interpolate between them: for a given number of interpolation steps $n$, we have:\n",
    "- In pixel space, the interpolated image $x_t$ at step $t=0,\\dots,n$ is given by taking, for each pixel, the linear interpolation \n",
    "$$\\frac{n-t}{n}x_1 + \\frac{t}{n}x_2.$$\n",
    "- In the latent space, the interpolated image $x_t$ at step $t=0,\\dots,n$ is given by first computing the linear interpolation $z_t$ between the encodings $z_1$ of $x_1$ and $z_2$ of $x_2$, and then decoding $z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the interpolate_pixel_space function\n",
    "# the function should take two images as input and the numer of interpolation steps\n",
    "# and plot the interpolated images in a single row\n",
    "\n",
    "x1, x2 = test_dataset[3][0], test_dataset[2][0]\n",
    "interpolate_pixel_space(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/pixel_interp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the interpolate_latent_space function\n",
    "# the function should take two images as input and the numer of interpolation steps\n",
    "# and plot the interpolated images in a single row\n",
    "\n",
    "x1, x2 = test_dataset[3][0], test_dataset[2][0]\n",
    "interpolate_latent_space(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/latent_interp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions.** \n",
    "1. How is the interpolation process any different?\n",
    "2. Are the first and last images the same for both interpolation processes? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the latent space\n",
    "The objective of this section is to visualize the latent space and to see how it changes according to which term in the loss function we give more weight to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite loss function to return BCE and KLD separately as well\n",
    "def loss_function(recon_x, x, mu, logvar, beta=1):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + beta * KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dims = 2\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "kl_weights = [1, 10, 100]  # Different weights for the KL divergence term\n",
    "\n",
    "# Training and plotting function\n",
    "def train_and_plot(kl_weight):\n",
    "    model = ConvVAE(latent_dims).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        bce_loss = 0\n",
    "        kld_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(data)\n",
    "            loss, bce, kld = loss_function(x_recon, data, mu, logvar, kl_weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            bce_loss += bce.item()\n",
    "            kld_loss += kld.item()\n",
    "        \n",
    "        average_loss = epoch_loss / len(train_loader.dataset)\n",
    "        average_bce = bce_loss / len(train_loader.dataset)\n",
    "        average_kld = kld_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}: Average Loss: {average_loss:.4f}, BCE: {average_bce:.4f}, KLD: {average_kld:.4f}')\n",
    "    \n",
    "    # Plot latent space\n",
    "    plot_latent_space(model, kl_weight)\n",
    "\n",
    "# Function to plot latent space\n",
    "def plot_latent_space(model, kl_weight):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=10000, shuffle=False)\n",
    "        data, labels = next(iter(test_loader))\n",
    "        data = data.to(device)\n",
    "        mu, logvar = model.encode(data)\n",
    "        z = mu  # For visualization, we use the mean\n",
    "        z = z.cpu().numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        plt.figure(figsize=(8,6))\n",
    "        scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(scatter, ticks=range(10))\n",
    "        plt.clim(-0.5, 9.5)\n",
    "        plt.title(f'Latent Space with KL Weight = {kl_weight}')\n",
    "        plt.xlabel('Z1')\n",
    "        plt.ylabel('Z2')\n",
    "        plt.show()\n",
    "\n",
    "# Run training and plotting for different KL weights\n",
    "for kl_weight in kl_weights:\n",
    "    print(f'\\nTraining VAE with KL Weight = {kl_weight}')\n",
    "    train_and_plot(kl_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** \n",
    "1. Describe is the effect of the KL weight $\\beta$ on the latent space.\n",
    "2. Explain why the described effect happens, and link it with the objective of each of the terms in the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
