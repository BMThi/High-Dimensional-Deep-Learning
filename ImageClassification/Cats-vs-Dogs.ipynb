{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification on [Cats-vs-Dogs](https://www.kaggle.com/competitions/dogs-vs-cats) dataset\n",
    "\n",
    "_Image classification models in a data-poor context_\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial is highly inspired by the [blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) from [François Chollet](https://fchollet.com/)  at the initiative of [`Keras`](https://keras.io/).\n",
    "\n",
    "In this tutorial you will learn to : \n",
    "* Use convolutional networks to  build image classifiers on color images\n",
    "* Use pre-trained models ($\\texttt{VGG}$, $\\texttt{Inception}$, _etc._) to improve the accuracy of the results\n",
    "* Fine-Tuned pre-trained models\n",
    "<!--* Cross-validation-->\n",
    "\n",
    "In this tutorial, we focus on the (seemingly) simple problem of recognizing dogs and cats in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilils\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Maths - Stats\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "# Data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning Librairies\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These code lines allow you to check if your computer is using CPU or GPU ressources. <br>\n",
    "**Warning** : You won't be able to use GPU if another notebook is open and still uses GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"GPU\" if \"GPU\" in [k.device_type for k in device_lib.list_local_devices()] else \"CPU\"\n",
    "print(MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in this TP is the [$\\texttt{Cats-vs-Dogs}$](https://www.kaggle.com/competitions/dogs-vs-cats) dataset used in a [Kaggle Contest](https://www.kaggle.com/c/dogs-vs-cats) which contains 25.000 images. It is a huge number when you do not have a lot of computation power. \n",
    "\n",
    "As our goal here is to understand behaviour of algorithms and not to achieve the best performances, we have created a subsample of this dataset which are available following repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://plmlab.math.cnrs.fr/chevallier-teaching/datasets/cats-vs-dogs.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are organized this way :\n",
    "\n",
    "```\n",
    "cats-vs-dogs\n",
    "└───train/\n",
    "│   └───cats/\n",
    "│   │   │   cat.0.jpg\n",
    "│   │   │   cat.1.jpg\n",
    "│   │   │   ...\n",
    "│   └───dogs/\n",
    "│   │   │   dog.0.jpg\n",
    "│   │   │   dog.1.jpg\n",
    "│   │   │   ...\n",
    "│   validation/\n",
    "│   │   cat.1500.jpg\n",
    "│   │   cat.1501.jpg\n",
    "│   │   ...\n",
    "│   │   dog.1500.jpg\n",
    "│   │   dog.1501.jpg\n",
    "│   │   ...\n",
    "│   test/\n",
    "│   │   cat.1000.jpg\n",
    "│   │   cat.1001.jpg\n",
    "│   │   ...\n",
    "│   │   dog.1000.jpg\n",
    "│   │   dog.1001.jpg\n",
    "│   │   ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./cats-vs-dogs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells can be used to load data in a suitable format:\n",
    "* **Step 1**: Creation of $\\texttt{lists}$ containing the image names of the training, validation and test sets, as well as the associated labels (0: cat, 1:dog);\n",
    "* **Step 2**: Creation of $\\texttt{dataframes}$ to enable data to be loaded as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Creation of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training images\n",
    "train_filenames_dogs = os.listdir(path + \"train/dogs\")\n",
    "train_filenames_cats = os.listdir(path + \"train/cats\")\n",
    "if not os.path.exists(path + \"train/train\"):\n",
    "    os.mkdir(path + \"train/train\")\n",
    "\n",
    "path_train = path + \"train/\"\n",
    "for filename in train_filenames_cats:\n",
    "    shutil.copyfile(path_train+\"cats/\"+filename, path_train+\"train/\"+filename)\n",
    "for filename in train_filenames_dogs:\n",
    "    shutil.copyfile(path_train+\"dogs/\"+filename, path_train+\"train/\"+filename)\n",
    "\n",
    "train_filenames = os.listdir(path + \"train/train\")\n",
    "train_categories =[]\n",
    "for filename in train_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        train_categories.append(1)\n",
    "    else:\n",
    "        train_categories.append(0)\n",
    "\n",
    "\n",
    "# Validation images\n",
    "validation_filenames = os.listdir(path +\"validation/\")\n",
    "validation_categories=[]\n",
    "for filename in validation_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        validation_categories.append(1)\n",
    "    else:\n",
    "        validation_categories.append(0)\n",
    "\n",
    "\n",
    "# Test images\n",
    "test_filenames = os.listdir(path + \"test/\")\n",
    "test_categories = []\n",
    "for filename in test_filenames:\n",
    "    category = filename.split('.')[0]\n",
    "    if category == 'dog':\n",
    "        test_categories.append(1)\n",
    "    else:\n",
    "        test_categories.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Creation of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training images\n",
    "total_train_df = pd.DataFrame({\n",
    "    'filename': train_filenames,\n",
    "    'category': train_categories\n",
    "})\n",
    "\n",
    "\n",
    "# Validation images\n",
    "total_validation_df = pd.DataFrame({\n",
    "    'filename': validation_filenames,\n",
    "    'category': validation_categories\n",
    "})\n",
    "\n",
    "\n",
    "# Test images\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames,\n",
    "    'category': test_categories\n",
    "})\n",
    "\n",
    "\n",
    "total_train_df['category'] = total_train_df['category'].astype(str)\n",
    "total_validation_df['category'] = total_validation_df['category'].astype(str)\n",
    "test_df['category'] = test_df['category'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question:** How many training, validation and test images are available in this dataset?</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/data_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question**: Is the data set balanced?</i>\n",
    "\n",
    "In other words, do each of the train, test and validation sets contain as many cat images as dog images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/data_ratio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large size of this dataset, we may need to consider only a subsample of it, to speed up training. \n",
    "Two parameters, $N_\\text{train}$ and $N_\\text{validation}$, iindicate respectively the number of training and validation data to be taken into account for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial = 0\n",
    "\n",
    "if partial:\n",
    "    # Even numbers required\n",
    "    N_train = 200   # 1000, 2000\n",
    "    N_validation = 100   # 500, 1000\n",
    "else:\n",
    "    N_train = total_train\n",
    "    N_test = total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if N_train == total_train:\n",
    "    train_df = total_train_df\n",
    "    validation_df = total_validation_df\n",
    "else:\n",
    "    N = int(.5*N_train)\n",
    "    train_df = pd.concat([total_train_df[total_train_df['category']=='1'][:N], \n",
    "                          total_train_df[total_train_df['category']=='0'][:N]])\n",
    "    train_df.sort_index(inplace=True)\n",
    "    N = int(.5*N_validation)\n",
    "    validation_df = pd.concat([total_validation_df[total_validation_df['category']=='1'][:N], \n",
    "                          total_validation_df[total_validation_df['category']=='0'][:N]])\n",
    "    del N\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The $\\texttt{load\\_img}$ function allows to load an image as a [PIL](https://he-arc.github.io/livre-python/pillow/index.html) image.\n",
    "* The function $\\texttt{img\\_to\\_array}$ generates an numpy array from a  PIL image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = rd.choice(train_df['filename'])\n",
    "img = load_img(path + \"train/train/\" + filename)\n",
    "\n",
    "display(img)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"PIL image\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "x = img_to_array(img)\n",
    "plt.imshow(x/255, interpolation='nearest')\n",
    "plt.title(\"Numpy image\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question**: What are the dimensions of the $x$ array?</i>\n",
    "\n",
    "To what correspond these dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question**: Does the shape of the images fluctuate?</i>\n",
    "\n",
    "This question can be answered by producing a boxplot of widths and heights respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/data_shape.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images have various dimensions, which is annoying because all images must have the same dimension to be used in this network. \n",
    "Next, we will impose a common image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\texttt{ImageDataGenerator}$ `keras`function allows to apply different treatments on the images (transformation, normalisation). This transformation allows to produce tranformation on the images without saving a lot of changed images on the disk. \n",
    "\n",
    "The syntax below defines _\"generators\"_ for each dataset, which load a predefined number of images (this will be our batch size) from the $\\texttt{dataframes}$ defined above.\n",
    "\n",
    "We also define a target image size (here, $150\\times150$) and a normalization pre-processing (division by 255).\n",
    "<br><br>\n",
    "\n",
    "\n",
    "> **Remark**: _Choice of $\\,\\texttt{batch\\_size}$_ <br>\n",
    "> When using keras generator, size of the batch should be a divider of the size of the sample, otherwise algorithms produce very unstable results in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20  # divider of N_train and N_validation\n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "# Training images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    path + \"train/train/\",\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    target_size = (img_width, img_height),\n",
    "    class_mode = 'binary',\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# Validation images\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validation_df,\n",
    "    path + \"validation/\",\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode = 'binary',\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# Test images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    path + \"test/\",\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode = 'binary',\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= {0: 'Cat', 1: 'Dog'}\n",
    "labels.get(0), labels.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise** : View a sample of images.</i>\n",
    "\n",
    "Select 9 images from the training dataset, and display them with their respective labels as titles. The code below shows an example of how to use the generators defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in train_generator:\n",
    "    print(x_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "    print(y_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "[...]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/data_visualization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## First approach: Basic convolutional network\n",
    "\n",
    "We will here build a classifier with a custom architecture of a convolutional network.\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "The images have all been resized to $150\\times150$. We can therefore define a convolutional neural network following this scheme:\n",
    "In the first phase, this network alternates between convolution and Max Pooling layers (in order to divide the dimension of the tensors by 2 each time).\n",
    "\n",
    "<center><img src=\"img/CatsDogsCNN.png\" style=\"width:700;height:350px;\"></center>\n",
    "<caption><center><b> View of architecture to be implemented </b></center></caption>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The model we define is composed of 3 convolution blocks with the following form: \n",
    "* A $\\texttt{Conv2D}$ layer with $3\\times3$ filters and a $\\texttt{Relu}$ activation function.<br>\n",
    "The first layer will have 32 convolution filters, the second 64, the third 96 (and the fourth 128).\n",
    "* A $\\texttt{MaxPooling}$ layer with $2\\times2$ window.\n",
    "\n",
    "Followed by:\n",
    "* A $\\texttt{flatten}$ layer.\n",
    "* A $\\texttt{Dense}$ layer with 64 (or 512) neurons and a Relu activation function.\n",
    "* A $\\texttt{Dropout}$ layer with a 50% drop rate.\n",
    "* A $\\texttt{Dense}$ layer with 1 neuron and a $\\texttt{softmax}$ activation function.\n",
    "\n",
    "You will have built a 6-layer network, a simplified version of $\\texttt{AlexNet}$.\n",
    "\n",
    "To build this network, you can use the $\\texttt{Conv2D}$, $\\texttt{Maxpooling2D}$ and $\\texttt{Flatten}$ functions in `Keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "cnn_simple = Sequential()\n",
    "\n",
    "# cnn_simple.add(Input(...))\n",
    "# cnn_simple.add(Conv2D(...))\n",
    "# cnn_simple.add(MaxPooling2D(..))\n",
    "# ...\n",
    "# cnn_simple.add(Flatten())    # Vectorization of the tensor to connect it to a dense layer\n",
    "# ...\n",
    "\n",
    "cnn_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/CNN_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "As our problem here is a two classes classifier we will use the $\\texttt{binary\\_crossentropy}$ loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_simple.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(learning_rate=3e-4),\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\texttt{batch\\_size}$:\n",
    "Note that the batch size has already been defined when setting `keras` $\\texttt{ImageDataGenerator}$. Recall that when using keras generator, size of the batch should be a divider of the size of the sample, otherwise algorithms produce very unstable results.\n",
    "\n",
    "So, we only need to define a number of epochs.\n",
    "\n",
    "\n",
    "* $\\texttt{epochs}$:\n",
    "Start with a small number (5-10) in order to check that computing time is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "t_learning_cnn_simple = time.time()\n",
    "cnn_simple_history = cnn_simple.fit(\n",
    "    train_generator,\n",
    "    validation_data = validation_generator,\n",
    "    epochs = epochs\n",
    ")\n",
    "t_learning_cnn_simple = time.time() - t_learning_cnn_simple\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_cnn_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_cnn_simple = time.time()\n",
    "\n",
    "score_cnn_train = cnn_simple.evaluate(train_generator, verbose=1)\n",
    "score_cnn_validation = cnn_simple.evaluate(validation_generator, verbose=1)\n",
    "\n",
    "t_prediction_cnn_simple = time.time() - t_prediction_cnn_simple\n",
    "\n",
    "print('Train accuracy:', score_cnn_train[1])\n",
    "print('Validation accuracy:', score_cnn_validation[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_cnn_simple )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise**: Visualize the evolution of metrics during training.</i>\n",
    "\n",
    "Write a function to display the evolution of metrics during training, on the training and validation sets. Display accuracy and loss on separate figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "def plot_training_analysis():\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/plot_training_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_analysis(cnn_simple_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question** : What phenomenon are you dealing with?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting correction \n",
    "\n",
    "With the previous network, we were in a typical **overfitting** situation. This is a classic problem when working with small databases in deep learning.\n",
    "Actually, the network you have created normally contains several million parameters (if you've followed the instructions). The problem you are trying to solve during training is establishing 3 million parameters with just 2,000 examples: that is too few!\n",
    "\n",
    "To limit this overfitting, we can apply the regularization techniques. In image processing, one of the most commonly used techniques is **data augmentation**.\n",
    "\n",
    "We are going to take the $\\texttt{ImageDataGenerator}$ created earlier to normalize images and use it to apply additional transformations to the images in our database. You can refer to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to find out what the parameters below correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rotation_range = 40,\n",
    "    rescale = 1./255,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_dataframe(\n",
    "    train_df,\n",
    "    path + 'train/train/',\n",
    "    x_col ='filename',\n",
    "    y_col ='category',\n",
    "    target_size=(img_width,img_height),\n",
    "    class_mode = 'binary',\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Question**: Why do we apply different transformations for learning and validation?</i>\n",
    "\n",
    "Indeed, we do not define a new generator for the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you can view images that have passed through our data augmentation loop. Observe how missing values in the images (for example, in the case of rotation) are filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "example_x, example_y = next(train_generator_augmented)\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(example_x[i])\n",
    "    plt.title(labels.get(example_y[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "We can now rebuild our model and start training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "cnn_simple = Sequential()\n",
    "[...]\n",
    "\n",
    "cnn_simple.summary()\n",
    "\n",
    "# --- #\n",
    "\n",
    "### TO BE COMPLETED ###\n",
    "epochs = 10\n",
    "\n",
    "cnn_simple.compile(...)\n",
    "\n",
    "t_learning_cnn_simple_augmented = ...\n",
    "cnn_simple_augmented_history = cnn_simple.fit(\n",
    "    train_generator_augmented,\n",
    "    validation_data = validation_generator,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_cnn_simple_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/CNN_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/CNN_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_cnn_simple_augmented = time.time()\n",
    "\n",
    "score_cnn_train_augmented = cnn_simple.evaluate(train_generator, verbose=1)\n",
    "score_cnn_validation_augmented = cnn_simple.evaluate(validation_generator, verbose=1)\n",
    "\n",
    "t_prediction_cnn_simple_augmented = time.time() - t_prediction_cnn_simple_augmented\n",
    "\n",
    "print('Train accuracy:', score_cnn_train_augmented[1])\n",
    "print('Validation accuracy:', score_cnn_validation_augmented[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_cnn_simple_augmented )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_analysis(cnn_simple_augmented_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise**: Compare the accuracy and loss values for training and validation to the ones observed in the last epochs of training.</i>\n",
    "\n",
    "* What do you observe?\n",
    "* Is this normal ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves clearly show that overfitting has been limited. Note also, and this is important, that training is slower: the model takes longer to correctly predict the training set. This is to be expected, as we have somehow \"complicated the problem\" by introducing all these deformations to our images.\n",
    "This form of \"data-driven\" regularization is in addition to the other methods such as L1/L2 regularization of network weights and Dropout.  \n",
    "\n",
    "One should now achieve around 75% accuracy on the validation set, which is good but not completely satisfactory: to continue improving, you will probably need to train longer but also have more data at your disposal.\n",
    "\n",
    "Another solution is to use **Transfer Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pre-trained Network\n",
    "\n",
    "We have seen above that the complexity of the data makes it difficult to build quickly an efficient classifier from scratch even  with an elaborate method as a convolutional network.\n",
    "One reason why our results were disappointing is that the first layers of our convolutional network, which are supposed to detect features useful for discriminating between dogs and cats, didn't learn sufficiently general filters from the 2000 training images. So, even if these filters are relevant for the 2000 training images, there is little chance that they will work well for generalization on new data.\n",
    "\n",
    "This is why we want to reuse a **pre-trained network** on a large database, enabling us to detect features that will generalize better to new data. These models are models that are very complex (see image below). They have been trained on a very huge amount of image data in order to classify them. \n",
    "\n",
    "The figure below represents a $\\texttt{VGG-16}$. This model is composed of _5 convolutional blocks_ which allows to build features on the images. The last block is a _fully convolutional block_. This last block can be seen as a simple _MLP model_ which is used on the features build by the convolutional block.\n",
    "\n",
    "<center><img src=\"https://blog.keras.io/img/imgclf/vgg16_original.png\" style=\"height:700px;\"></center>\n",
    "<caption><center><b> VGG-16 </b></center></caption>\n",
    "\n",
    "\n",
    "How this model, designed to solve a different problem that our problem can be helpfull?\n",
    "\n",
    "Here is our two-stage strategy :  \n",
    "1. **Features map**: We will send our data through the 5 convolutional blocks in order to build features. These blocks have been trained on a huge amount of data and can then build intelligent features.\n",
    "2. **MLP classifier**: We will build our own MLP classifier designed to solve our Cats-vs-Dogs problem, and we will train it on the features built on the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download  the weights of the 5 blocks convolutional layer.\n",
    "\n",
    "We will now download the weights of a VGG16 model that has been learned on the [$\\texttt{ImageNet}$](http://www.image-net.org) dataset, which is composed of millions of images for 1000 categories.\n",
    "\n",
    "If it's the first time you use these weights, you will have to download it (it will start automatically) and they will be save in your home \n",
    "`\"~/.keras/models\"`\n",
    "\n",
    "The $\\texttt{include\\_top}$ argument of the $\\texttt{VGG16}$ application allows to precise if we want to use or not the last block (fully-connected later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(\n",
    "    weights = 'imagenet', # We use the network weights already pre-trained on the ImageNet database.\n",
    "    include_top = False,  # The Dense part of the original network is not retained\n",
    "    input_shape = (img_width, img_height, 3)\n",
    ")\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the features, learned by the neural network on ImageNet, from our dataset of dog and cat images. Compared to the first part, the advantage is that it would have been almost impossible to deduce these “general” features (found in a huge database) from our too-small dataset of 2000 images. On the other hand, these general features should prove useful for our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building features\n",
    "\n",
    "The structure of the $\\texttt{VGG}$ network summarized by the $\\texttt{summary}$ function above shows that the output tensor is of dimension $4\\times4\\times512$, _i.e._ the network predicts features of dimension $4\\times4\\times512$ from an image of size $150\\times150$.\n",
    "\n",
    "Therefore, we must take care to vectorize this output so that we can pass it through a dense network.\n",
    "This can be achieved by vectorizing the features map upstream of the MLP classifier to follow; or by adding a “Flatten” layer to this dense network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = conv_base.predict(train_generator)\n",
    "train_features = np.reshape(train_features,(train_features.shape[0], -1))\n",
    "\n",
    "validation_features = conv_base.predict(validation_generator)\n",
    "validation_features = np.reshape(validation_features,(validation_features.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to retrieve the associated labels; we will seek them in the dataframe defined at the start of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.to_numpy()\n",
    "y_train = np.array([int(numeric_string) for numeric_string in train_data[:,1]])\n",
    "\n",
    "validation_data = validation_df.to_numpy()\n",
    "y_validation = np.array([int(numeric_string) for numeric_string in validation_data[:,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 :  Building our classifier on top of features\n",
    "\n",
    "We can now define a simple neural network that will work directly on the features predicted by $\\texttt{VGG}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise**: Write this classifier.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "vgg_mlp = Sequential()\n",
    "[...]\n",
    "\n",
    "vgg_mlp.summary()\n",
    "\n",
    "# --- #\n",
    "\n",
    "### TO BE COMPLETED ###\n",
    "epochs = 10\n",
    "\n",
    "vgg_mlp.compile(...)\n",
    "\n",
    "t_learning_vgg_mlp = ...\n",
    "vgg_mlp_history = vgg_mlp.fit(\n",
    "    train_features, y_train,\n",
    "    validation_data = (validation_features, y_validation),\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_vgg_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/MLP_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/MLP_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_vgg_mlp = time.time()\n",
    "\n",
    "score_vgg_mlp_train = vgg_mlp.evaluate(train_features, y_train)\n",
    "score_vgg_mlp_validation = vgg_mlp.evaluate(validation_features, y_validation)\n",
    "\n",
    "t_prediction_vgg_mlp = time.time() - t_prediction_vgg_mlp\n",
    "\n",
    "print('Train accuracy:', score_vgg_mlp_train[1])\n",
    "print('Validation accuracy:', score_vgg_mlp_validation[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_vgg_mlp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_analysis(vgg_mlp_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we observe a lot of overfitting. We need to find a way of integrating data augmentation.\n",
    "\n",
    "To do this, we can connect our small neural network to the end of the $\\texttt{VGG}$ convolutional base. The idea is that by reusing our augmented data generator, we will be able to compute $\\texttt{VGG}$ features on the augmented data, and thus classify these features rather than features from our database alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning combined with Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We start by creating a new model based on $\\texttt{VGG}$'s convolutional base, to which we add a dense layer and our output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_combined = Sequential()\n",
    "vgg_combined.add(Input(shape=(img_width, img_height, 3)))\n",
    "vgg_combined.add(conv_base)\n",
    "vgg_combined.add(Flatten())\n",
    "vgg_combined.add(Dense(256, activation='relu'))\n",
    "vgg_combined.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "vgg_combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**: It is important to avoid training $\\texttt{VGG}$'s convolutional base! We do not want to override the good features of VGG that we are trying to reuse! The network would also have a large number of parameters, which is precisely what we want to avoid.\n",
    "\n",
    "To do this, we can use the $\\texttt{trainable}$ attribute: by setting it to $\\texttt{False}$, we can freeze the weights and prevent them from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "vgg_combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the number of weights: the number of trainable weights is now 2 million, compared with 16 million previously; we are only going to train the weights of our dense layer and the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "vgg_combined.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(learning_rate=3e-4),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "t_learning_vgg_combined = time.time()\n",
    "vgg_combined_history = vgg_combined.fit(\n",
    "    train_generator_augmented,\n",
    "    validation_data = validation_generator,\n",
    "    epochs = epochs\n",
    ")\n",
    "t_learning_vgg_combined = time.time() - t_learning_vgg_combined\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_vgg_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is much slower! We have to generate the augmented data, and feed it through the VGG layers at each gradient iteration. This takes time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_vgg_combined = time.time()\n",
    "\n",
    "score_vgg_combined_train = vgg_combined.evaluate(train_generator_augmented)\n",
    "score_vgg_combined_validation = vgg_combined.evaluate(validation_generator)\n",
    "\n",
    "t_prediction_vgg_combined = time.time() - t_prediction_vgg_combined\n",
    "\n",
    "print('Train accuracy:', score_vgg_combined_train[1])\n",
    "print('Validation accuracy:', score_vgg_combined_validation[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_vgg_combined )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_analysis(vgg_combined_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, overfitting has been limited, which was the aim. This considerably improves results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "We have notably increased the performances of our model with a model that is really efficient. We can continue to try to improve our results by modifying the small MLP classifier network we build. \n",
    " \n",
    "But to really improve our model, it would be nice to also change the weights of the previous layers in order to make them fit our problem. This is possible and called **Fine Tuning**.\n",
    "To do this, we are going to start again from the network we just trained, but will unlock the training of all or part of the weights of the entire network.\n",
    "<br><br>\n",
    "\n",
    "> **WARNING**: It is important to choose a very low learning rate so as not to wipe out the benefits of previous training sessions.\n",
    "The aim is simply to evolve the network parameters “at the margin”, and this can only be done after the first *transfer learning* step. Otherwise, the last layers added after the convolutional base, after their random initialization, would have generated strong gradients that would have completely destroyed the general VGG filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will then build a Model which is composed of the 5 convolutional block of the $\\texttt{VGG}$ model (with its weights learned on $\\texttt{ImageNet}$) and the classifier block we built (with the weights that we have learned previously).\n",
    "\n",
    "<center><img src=\"https://blog.keras.io/img/imgclf/vgg16_modified.png\" style=\"height:700px;\"></center>\n",
    "<caption><center><b> VGG-16 </b></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We will start by fine-tune only the last block of convolution of our classifier. To that aim, we start by reactivating $\\texttt{VGG}$ convolutional base parameter training. We then update the trainable arguments of the layers that we don't want to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "for layer in conv_base.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "vgg_combined.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(learning_rate=1e-4), # Reduced learning rates to avoid smashing everything and risking overfitting\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "t_learning_vgg_combined_tuned = time.time()\n",
    "vgg_combined_tuned_history = vgg_combined.fit(\n",
    "    train_generator_augmented,\n",
    "    validation_data = validation_generator,\n",
    "    epochs = epochs\n",
    ")\n",
    "t_learning_vgg_combined_tuned = time.time() - t_learning_vgg_combined_tuned\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_vgg_combined_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_vgg_combined_tuned = time.time()\n",
    "\n",
    "score_vgg_combined_tuned_train = vgg_combined.evaluate(train_generator_augmented)\n",
    "score_vgg_combined_tuned_validation = vgg_combined.evaluate(validation_generator)\n",
    "\n",
    "t_prediction_vgg_combined_tuned = time.time() - t_prediction_vgg_combined_tuned\n",
    "\n",
    "print('Train accuracy:', score_vgg_combined_tuned_train[1])\n",
    "print('Validation accuracy:', score_vgg_combined_tuned_validation[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_vgg_combined_tuned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_analysis(vgg_mlp_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training again\n",
    "\n",
    "We decide to continue fine-tuning the entire convolutional base, with an even smaller step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "vgg_combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "vgg_combined.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(learning_rate=1e-5), # Reduced learning rates to avoid smashing everything and risking overfitting\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "t_learning_vgg_combined_tuned2 = time.time()\n",
    "vgg_combined_tuned_history2 = vgg_combined.fit(\n",
    "    train_generator_augmented,\n",
    "    validation_data = validation_generator,\n",
    "    epochs = epochs\n",
    ")\n",
    "t_learning_vgg_combined_tuned2 = time.time() - t_learning_vgg_combined_tuned2\n",
    "\n",
    "print(\"Learning time for %d epochs : %d seconds\" % (epochs, t_learning_vgg_combined_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prediction_vgg_combined_tuned2 = time.time()\n",
    "\n",
    "score_vgg_combined_tuned_train2 = vgg_combined.evaluate(train_generator_augmented)\n",
    "score_vgg_combined_tuned_validation2 = vgg_combined.evaluate(validation_generator)\n",
    "\n",
    "t_prediction_vgg_combined_tuned2 = time.time() - t_prediction_vgg_combined_tuned2\n",
    "\n",
    "print('Train accuracy:', score_vgg_combined_tuned_train2[1])\n",
    "print('Validation accuracy:', score_vgg_combined_tuned_validation2[1])\n",
    "print(\"Time Prediction: %.2f seconds\" % t_prediction_vgg_combined_tuned2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_analysis(vgg_combined_tuned_history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Kaggle Dataset\n",
    "\n",
    "Let's see now how our trained model performs on the kaggle real test dataset ($\\texttt{cats-vs-dogs/test}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise**: Apply the model to this dataset and display results on a sample to check it performs well</i>\n",
    "\n",
    "Predicted labels will be displayed as chart titles, colored green if the prediction is correct and red if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/CatsDogs/test_kaggle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling\n",
    "\n",
    "Actually, we no longer really use the $\\texttt{Flatten}$ layer to bridge the gap between convolutional and dense layers, but rather a [$\\texttt{GlobalAveragePooling}$](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) layer. Try to understand what this layer does and modify the network built on top of $\\texttt{VGG}$ accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED ###\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/GlobalAveragePooling_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/GlobalAveragePooling_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/GlobalAveragePooling_tune.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Keras has a lot of pre-trained model\n",
    "\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "\n",
    "Some have a much more complex architecture like `InceptionV3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <i style=\"color:purple\">**Exercise**: Restart the TP by using a different pre-trained model and apply the required modifications.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "865px",
    "left": "0px",
    "right": "1587.01px",
    "top": "106px",
    "width": "213px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
