{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97a1ebd-df7e-4f37-81c1-7730707a1da6",
   "metadata": {},
   "source": [
    "# **Correction**: Object Detection Using Deep learning\n",
    "---\n",
    "\n",
    "Object detection is a task in computer vision that involves identifying the presence, location, and type of one or more objects in a given photograph. \n",
    "It is a challenging problem that involves building upon methods for object recognition (_e.g._ where are they), object localization (_e.g._ what are their extent), and object classification (_e.g._ what are they).\n",
    "\n",
    "In recent years, deep learning techniques are achieving state-of-the-art results for object detection, such as on standard benchmark datasets and in computer vision competitions. Notable is the “You Only Look Once”, or YOLO, family of Convolutional Neural Networks that achieve near state-of-the-art results with a single end-to-end model that can perform object detection in real-time. \n",
    "\n",
    "In this lab, we will first focus on region-based convolutional neural networks. Then, based on the pre-trained YOLO model “<a href=\"https://github.com/experiencor/keras-yolo3\">keras-yolo3: Training and Detecting Objects with YOLO3</a>” developed by Huynh Ngoc Anh, we will develop a YOLO3 model for object detection on new photographs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf786153-2330-4d0a-abbe-ee38a2032985",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa51e93-aecf-4fce-9feb-4928165fa445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc6f854d-62d7-4387-8910-c7dad8b43602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "import numpy as np \n",
    "from numpy import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e4d355-7111-4ee7-81eb-a338f191b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  # Output network weights in HDF5 format during checkpointing\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b95bdb2-ee2e-4e4e-967a-f2fe2d778929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a62e890-b925-4e20-b536-3f869bea7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a25604-b2ae-47ed-8c08-7e385be48594",
   "metadata": {},
   "source": [
    "---\n",
    "# PART I: Region-Based Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c4764-337c-4607-86e8-073bc6ed8b56",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "\n",
    "For this lab, we will work on a health-related dataset: our goal is to solve a blood cell detection problem. Specifically, our task is to detect all red blood cells (RBCs), white blood cells (WBCs), and platelets in images taken via microscopic image readings. Below is an example of what our final predictions should look like:\n",
    "\n",
    "<img src=\"myBCCD/example.png\" width=\"400\" />\n",
    "\n",
    "We chose this data set because the density of RBCs, WBCs and platelets in our blood stream provides a lot of information about our immune system and hemoglobin. This can help us identify whether a person is healthy or not, and if an abnormality is found in their blood, steps can be taken quickly to diagnose it.\n",
    "\n",
    "Manually reviewing such samples under a microscope is a tedious process. This is where deep learning models play a key role. They can classify and detect blood cells from microscopic images with impressive accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "The full blood cell detection dataset for this lab can be downloaded from [github.com/Shenggan/BCCD_Dataset](https://github.com/Shenggan/BCCD_Dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef377c-7258-4929-8760-5f6b75d71fa7",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "\n",
    "From the whole dataset, I created different files that you can find on moodle. First, I created the division between training set and test set, by randomly choosing images from the whole dataset as training data. In particular:\n",
    "1. `train_images`: Images we will use to train the model,\n",
    "2. `test_images`: Images that will be used to make predictions using the trained model,\n",
    "3. `train.csv`: Contains the name, class and coordinates of the bounding box for each image. There can be multiple rows for an image because a single image can contain more than one object.\n",
    "\n",
    "Data exploration helps us not only to discover hidden patterns, but also to get a valuable global view of what we are working on. Take a look at our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895966e3-091f-4ee2-8df8-0ba68f8b3dcd",
   "metadata": {},
   "source": [
    "---\n",
    "### train.csv\n",
    "\n",
    "First, let's read the `train.csv` file (if you feel like “wasting\" time or experimenting, you can create your own $\\texttt{.csv}$ file from the original data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e87d8b-1d5d-4012-b0a3-9117833ee7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>278</td>\n",
       "      <td>366</td>\n",
       "      <td>196</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>359</td>\n",
       "      <td>447</td>\n",
       "      <td>239</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>523</td>\n",
       "      <td>611</td>\n",
       "      <td>110</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>447</td>\n",
       "      <td>568</td>\n",
       "      <td>198</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>183</td>\n",
       "      <td>304</td>\n",
       "      <td>95</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>164</td>\n",
       "      <td>268</td>\n",
       "      <td>201</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>191</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>90</td>\n",
       "      <td>194</td>\n",
       "      <td>171</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>496</td>\n",
       "      <td>600</td>\n",
       "      <td>304</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>271</td>\n",
       "      <td>375</td>\n",
       "      <td>338</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>157</td>\n",
       "      <td>261</td>\n",
       "      <td>370</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>311</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>104</td>\n",
       "      <td>208</td>\n",
       "      <td>285</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>WBC</td>\n",
       "      <td>305</td>\n",
       "      <td>521</td>\n",
       "      <td>13</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BloodImage_00301.jpg</td>\n",
       "      <td>Platelets</td>\n",
       "      <td>592</td>\n",
       "      <td>630</td>\n",
       "      <td>277</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>349</td>\n",
       "      <td>499</td>\n",
       "      <td>120</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>343</td>\n",
       "      <td>437</td>\n",
       "      <td>272</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>184</td>\n",
       "      <td>283</td>\n",
       "      <td>292</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>253</td>\n",
       "      <td>352</td>\n",
       "      <td>240</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>408</td>\n",
       "      <td>505</td>\n",
       "      <td>282</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>143</td>\n",
       "      <td>240</td>\n",
       "      <td>189</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>305</td>\n",
       "      <td>404</td>\n",
       "      <td>43</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>373</td>\n",
       "      <td>472</td>\n",
       "      <td>21</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>458</td>\n",
       "      <td>559</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BloodImage_00103.jpg</td>\n",
       "      <td>RBC</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>197</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_name  cell_type  xmin  xmax  ymin  ymax\n",
       "0   BloodImage_00301.jpg        RBC   278   366   196   291\n",
       "1   BloodImage_00301.jpg        RBC   359   447   239   334\n",
       "2   BloodImage_00301.jpg        RBC   523   611   110   205\n",
       "3   BloodImage_00301.jpg        RBC   447   568   198   309\n",
       "4   BloodImage_00301.jpg        RBC   183   304    95   206\n",
       "5   BloodImage_00301.jpg        RBC   164   268   201   307\n",
       "6   BloodImage_00301.jpg        RBC   191   295     1   105\n",
       "7   BloodImage_00301.jpg        RBC    90   194   171   277\n",
       "8   BloodImage_00301.jpg        RBC   496   600   304   410\n",
       "9   BloodImage_00301.jpg        RBC   271   375   338   444\n",
       "10  BloodImage_00301.jpg        RBC   157   261   370   476\n",
       "11  BloodImage_00301.jpg        RBC     1   104   311   417\n",
       "12  BloodImage_00301.jpg        RBC   104   208   285   391\n",
       "13  BloodImage_00301.jpg        WBC   305   521    13   214\n",
       "14  BloodImage_00301.jpg  Platelets   592   630   277   314\n",
       "15  BloodImage_00103.jpg        RBC   349   499   120   206\n",
       "16  BloodImage_00103.jpg        RBC   343   437   272   357\n",
       "17  BloodImage_00103.jpg        RBC   184   283   292   378\n",
       "18  BloodImage_00103.jpg        RBC   253   352   240   326\n",
       "19  BloodImage_00103.jpg        RBC   408   505   282   393\n",
       "20  BloodImage_00103.jpg        RBC   143   240   189   290\n",
       "21  BloodImage_00103.jpg        RBC   305   404    43   129\n",
       "22  BloodImage_00103.jpg        RBC   373   472    21   107\n",
       "23  BloodImage_00103.jpg        RBC   458   559    50   160\n",
       "24  BloodImage_00103.jpg        RBC     1    94   197   285"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"myBCCD/train.csv\")\n",
    "train.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edafd5-3220-4b89-ab25-d5c7dfd923b2",
   "metadata": {},
   "source": [
    "There are 6 columns in the train file. Make sure you understand what each column represents.\n",
    "1. `image_name`: contains the name of the image\n",
    "2. `cell_type`: denotes the type of the cell\n",
    "3. `xmin`: x-coordinate of the bottom left part of the image\n",
    "4. `xmax`: x-coordinate of the top right part of the image\n",
    "5. `ymin`: y-coordinate of the bottom left part of the image\n",
    "6. `ymax`: y-coordinate of the top right part of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb00646-9264-4fba-99ec-8037d8457e14",
   "metadata": {},
   "source": [
    "---\n",
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d2215-f1fb-4fa0-a849-fbeaaec87c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('myBCCD/JPEGImages_train/BloodImage_00301.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3eb8df-effd-4dec-b616-40a50e66fa81",
   "metadata": {},
   "source": [
    "This is what a blood cell image looks like. Here, the blue part represents the WBCs, and the slightly red parts represent the RBCs. \n",
    "\n",
    "Let’s look at how many images, and the different type of classes, there are in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653be8c-c79e-472b-875b-d421d3912dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique training images\n",
    "print('Training images: {:.0f}'.format(train['image_name'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a5ec0d4-934d-4bee-992c-2bf137201d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RBC          3458\n",
       "WBC           307\n",
       "Platelets     298\n",
       "Name: cell_type, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes\n",
    "train['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d1342-1944-4898-95c1-4553b37dfec2",
   "metadata": {},
   "source": [
    "So, we have 300 training images and three different classes of cells, _i.e._ RBC, WBC and Platelets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad4179-8b45-41f1-a11f-5803b4d1d7af",
   "metadata": {},
   "source": [
    "---\n",
    "### Image with Detected Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf328a-0692-47cf-819f-da44628faff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "#add axes to the image\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "# read and plot the image\n",
    "img = plt.imread('myBCCD/JPEGImages_train/BloodImage_00301.jpg')\n",
    "plt.imshow(img)\n",
    "\n",
    "# iterating over the image for different objects\n",
    "for _,row in train[train.image_name == \"BloodImage_00301.jpg\"].iterrows():\n",
    "    xmin = row.xmin\n",
    "    xmax = row.xmax\n",
    "    ymin = row.ymin\n",
    "    ymax = row.ymax\n",
    "    \n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    \n",
    "    # assign different color to different classes of objects\n",
    "    if row.cell_type == 'RBC':\n",
    "        edgecolor = '#b77cb8'\n",
    "        ax.annotate('RBC', xy=(xmax-40,ymin+20))\n",
    "    elif row.cell_type == 'WBC':\n",
    "        edgecolor = '#709ab3'\n",
    "        ax.annotate('WBC', xy=(xmax-40,ymin+20))\n",
    "    elif row.cell_type == 'Platelets':\n",
    "        edgecolor = '#5aaa80'\n",
    "        ax.annotate('Platelets', xy=(xmax-40,ymin+20))\n",
    "        \n",
    "    # add bounding boxes to the image\n",
    "    rect = patches.Rectangle((xmin,ymin), width, height, edgecolor=edgecolor, facecolor='none', linewidth=1.5)\n",
    "    \n",
    "    ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccf751-92b0-4318-a295-fbf6f37473a0",
   "metadata": {},
   "source": [
    "This is what a training example looks like. We have the different classes and their corresponding bounding boxes. Let’s now train our model on these images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a06f6b-4270-4647-afd5-feef22ddf833",
   "metadata": {},
   "source": [
    "---\n",
    "## R-CNN\n",
    "\n",
    "In 2014, [Girshick et al.](https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) proposed `R-CNN` (Region-based Convolutional Neural Networks).  \n",
    "\n",
    "From an input image, ~2000 bounding boxes are generated using [`Selective Search`](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf), these bounding-boxes represent region proposals (“Region of Interest\" or **RoI**). \n",
    "\n",
    "Those proposed regions are cropped and warped to a fixed size. A pre-trained ConvNet model (like `VGG` or `ResNet`) is then used to extract a feature vector for each warped region independently. \n",
    "\n",
    "Then an SVM model is trained to classify the object in the warped image using the feature vector. A regression layer is also trained to refine the bounding box proposals \n",
    "\n",
    "<!---![](https://www.deeplearningitalia.com/wp-content/uploads/2018/06/2.png )--->\n",
    "<img src=\"https://www.deeplearningitalia.com/wp-content/uploads/2018/06/2.png\" width=\"500\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "However, training `R-CNN` is expensive and slow. Running `Selective Search` to propose 2000 regions for every image and then generate the ConvNet fetatures for every region in the image, takes a lot of time. \n",
    "\n",
    "Let's see how long it will take to genereate the ConvNet features of 2000 region proposals. \n",
    "In the following code we will not take into account the time of the `Selective Search`, for this reason, we will simulate this process generating random regions of the image. Then we will count the time a pre-trained ConvNet like `ResNet` takes to generate the features.\n",
    "\n",
    "We will use Keras API to load the pre-trained `ResNet` and to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d094177-31c8-40b9-a982-79a2e054f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('myBCCD/JPEGImages_train/BloodImage_00301.jpg') ## Reading image\n",
    "img_h = img.shape[0]\n",
    "img_w = img.shape[1]\n",
    "nb_proposals = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ca7aa-a8a5-49eb-956c-43934b2f5e6c",
   "metadata": {},
   "source": [
    "The region proposals are small regions of the image. We will define this region as a set of 4 points $[x_0, y_0, x_1, y_1]$. For instance, if the image size is $500\\times500$, a region proposal might be $[0, 0, 224, 224]$ which is the $224\\times224$ top left corner area of the image. \n",
    "\n",
    "* Let's create random region proposals in order to simulate the `Selective Search` process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae9b1c-2f29-4dc8-bed0-471abd8af26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_props = []\n",
    "input_size = 224  # ResNet only accepts inputs of 224x224, to make it easier we will only generate region proposal of this size\n",
    "\n",
    "for i in range(nb_proposals):  \n",
    "    x_0 = rd.randint(0, img_w - input_size) \n",
    "    y_0 = rd.randint(0, img_h - input_size)\n",
    "    x_1 = x_0 + input_size\n",
    "    y_1 = y_0 + input_size\n",
    "    region_props.append([x_0, y_0, x_1, y_1])\n",
    "\n",
    "region_props[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16682c5c-7a6e-4ac1-825a-f1f16c765ccc",
   "metadata": {},
   "source": [
    "* Now let's generate the ConvNet features for each of this region proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd6406-1d14-4e87-a885-9f857acdcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = ResNet50(weights = 'imagenet')  # Load ResNet model pre-trained with ImageNet dataset\n",
    "\n",
    "t = time.time()\n",
    "batch_size = 100  # The CNN model processes batch_size number of images at the same time\n",
    "X = np.zeros((batch_size, input_size, input_size, 3))\n",
    "b = 0\n",
    "\n",
    "for prop in region_props:\n",
    "    img_to_pred = img[prop[1]:prop[3],prop[0]:prop[2]] # Select the proposal from the image\n",
    "    img_to_pred = np.expand_dims(img_to_pred, axis=0) # Add the batch dimension. From (input_size, input_size, 3) to (1, input_size, input_size, 3)\n",
    "    img_to_pred = preprocess_input(img_to_pred) # convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset\n",
    "    \n",
    "    if b == batch_size:\n",
    "        CNN_model.predict(X) # Extract features\n",
    "        b = 0\n",
    "        X = np.zeros((batch_size, input_size, input_size, 3))    \n",
    "    \n",
    "    X[b] = img_to_pred\n",
    "    b += 1\n",
    "\n",
    "print('Time to generate features for one image: {:.3f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62388ea-6154-485d-9b16-5461ba0f5dc9",
   "metadata": {},
   "source": [
    "---\n",
    "## Fast R-CNN\n",
    "\n",
    "To improve `R-CNN`,  [Girshick et al., 2015](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf) proposed `Fast R-CNN`. Instead of extracting ConvNet features independently for each region proposal, this model only needs one ConvNet forward pass per image since the features of the regions are extracted from the feature map of the entire image.\n",
    "\n",
    "<img src=\"https://www.deeplearningitalia.com/wp-content/uploads/2018/06/3-1.png\" width=\"500\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "The following steps summarize the method:\n",
    "\n",
    "1. Propose regions (“Region of Interest” or **RoI**) by `Selective Search`,\n",
    "\n",
    "2. Take a pre-trained ConvNet like `VGG` or `ResNet` and replace the last _Max Pooling layer_ by a _RoI Pooling layer_. <br> <span style=\"font-size:small;color:gray\">The inputs of the _RoI Pooling layer_ are the different region proposals projected in the feature map of the entire image and the output is a fixed-length feature vector per proposal. <br> In other words, if we compare `R-CNN` with `Fast R-CNN`, `R-CNN` extracts the region proposals from the original images, warpes them in a fixed size and passes each of these small images to the pre-trained ConvNet. On the other hand, `Fast-RCNN` passes only one time the entire image to the pre-trained ConvNet to obtain a feature map, then it extracts the region proposals from this feature map and use _RoI pooling_ to get fixed size regions.</span>\n",
    "\n",
    "3. Finally, from the fixed-length feature vector per proposal, the model branches into two output layers:\n",
    "    * A _softmax estimator_ where the output is a discrete probability distribution per _RoI_.\n",
    "    * A bounding-box _regression model_ which predicts offsets relative to the original _RoI_.\n",
    "\n",
    "<br>\n",
    "\n",
    "`Fast R-CNN` performs much better than `R-CNN` in terms of speed. There was just one big bottleneck remaining: the `Selective Search algorithm` for generating region proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ce700-b331-4f1e-86bb-5a0e0284517b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "### Remark: **RoI** Pooling\n",
    "\n",
    "It is a type of _max pooling_ which goal is to convert features of any size ($h\\times w$), into a small fixed window ($H\\times W$). The input region is divided into $H\\times W$ grids, approximately every subwindow of size $\\frac{h}H\\times\\frac{w}W$. Then apply _max-pooling_ in each grid.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*aB4gy6i8Zc3BasYaQGDVtg.png\" width=\"750\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "Now let's see how long it takes the Selective Search algorithm in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f257fa-0b63-4574-9995-8cc73e3a0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selectivesearch  # Selective Search algorithm taken from https://github.com/AlpacaDB/selectivesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23255c2-dbf7-4c00-9e93-32a7640946dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selectivesearch\n",
    "\n",
    "img = plt.imread('myBCCD/JPEGImages_train/BloodImage_00301.jpg') ## Reading image\n",
    "t = time.time()\n",
    "img_lbl, regions = selectivesearch.selective_search(img)\n",
    "\n",
    "print('Time for Selective search on one single image: {:.3f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5ff57-a62f-48b1-8c18-c3dffae58cb2",
   "metadata": {},
   "source": [
    "---\n",
    "## Faster R-CNN <span style=\"color:purple\">(To Go Further)</span>\n",
    "\n",
    "An intuitive speedup solution is to integrate the region proposal algorithm into the ConvNet model. `Faster R-CNN`  ([Ren et al., 2016](http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)) constructs a single, unified model composed of `RPN` (Region Proposal Network) and `Fast R-CNN` with shared convolutional feature layers. \n",
    "\n",
    "Instead of using `Selective Search`, _Ren et al._ proposed to let the network learn the region proposals using `RPN`.\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "`Faster RCNN` takes the feature maps from ConvNet and passes them on to the Region Proposal Network. `RPN` uses a sliding window over these feature maps, and at each window, it generates $k$ Anchor boxes of different shapes and sizes. More precisely:\n",
    "1. Take a pre-trained ConvNet like `VGG` or `ResNet`. At the last layer of the ConvNet, a $3\\times3$ sliding window moves across the feature map.\n",
    "2. At the center of each sliding window, it predicts multiple regions of various scales and ratios simultaneously. \n",
    "3. For each region proposal the model predicts: a score for that region, and 4 coordinates representing the bounding box of the region.\n",
    "\n",
    "The $2k$ scores represent the softmax probability of each of the $k$ bounding boxes being an “object”. Although the `RPN` outputs bounding box coordinates, it does not try to classify any potential object. Its real job is still proposing object regions. If an anchor box has a score above a certain threshold, that box passes forward as a region proposal.\n",
    "\n",
    "Once we have the region proposals, we get the fixed length feature vector using a _RoI pooling_ layer. These feature vectors are then used to classify the proposals and predict the offset values for the bounding boxes. In other workds, `Faster R-CNN = RPN + Fast R-CNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea48566-632a-4aef-bd9a-ef79a6e5b5f2",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### **Opt1**: Keras Github repository <span style=\"color:purple\">(To Go Further)</span>\n",
    "\n",
    "To implement the `Faster R-CNN` algorithm, you can try to follow the steps mentioned in the [Keras Github repository](https://github.com/kbardool/keras-frcnn) that implements `Faster R-CNN`. So as the first step, make sure you clone this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607a902-beab-4e10-9b14-ec574ef15890",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kbardool/keras-frcnn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20530185-82ac-47ff-b06a-d5a44d1073ec",
   "metadata": {},
   "source": [
    "In order to train the model on a new dataset, the format of the input should be:\n",
    "\n",
    "`filepath, x1, y1, x2, y2, class_name`\n",
    "\n",
    "where:\n",
    "* `filepath` is the path of the training image,\n",
    "* `x1` is the xmin coordinate for bounding box\n",
    "* `y1` is the ymin coordinate for bounding box\n",
    "* `x2` is the xmax coordinate for bounding box\n",
    "* `y2` is the ymax coordinate for bounding box\n",
    "* `class_name` is the name of the class in that bounding box\n",
    "\n",
    "See the `README.md` file for more details.\n",
    "\n",
    "<br>\n",
    "\n",
    "We need to convert the $\\texttt{.csv}$ format into a $\\texttt{.txt}$ file which will have the same format as described above. \n",
    "\n",
    "<!-- Make a new dataframe, fill all the values as per the format into that dataframe, and then save it as a .txt file. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc9f32-b640-4c03-838b-549c415462b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['format'] = train['image_name']\n",
    "\n",
    "# the images are in JPEGImages_train folder\n",
    "for i in range(data.shape[0]):\n",
    "    data['format'][i] = 'JPEGImages_train/' + data['format'][i]\n",
    "\n",
    "# add xmin, ymin, xmax, ymax and class as per the format required\n",
    "for i in range(data.shape[0]):\n",
    "    data['format'][i] = data['format'][i] + ',' + str(train['xmin'][i]) + ',' + str(train['ymin'][i]) + ',' + str(train['xmax'][i]) + ',' + str(train['ymax'][i]) + ',' + train['cell_type'][i]\n",
    "\n",
    "data.to_csv('annotate.txt', header=None, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda14133-5ad8-4d1b-9719-87e1a5e42372",
   "metadata": {},
   "source": [
    "Move the `JPEGImages_train` and `JPEGImages_test` folder, as well as the `annotate.txt` file, to the cloned repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5581e1f-256b-4be3-bb90-9a0de7e2f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"annotate.txt\" \"keras-frcnn/annotate.txt\"\n",
    "!cp -r \"myBCCD/JPEGImages_train\" \"keras-frcnn/JPEGImages_train\"\n",
    "!cp -r \"myBCCD/JPEGImages_test\" \"keras-frcnn/JPEGImages_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dedf4-3a23-4db7-a0f4-223acdb6db08",
   "metadata": {},
   "source": [
    "You can now train your model !\n",
    "\n",
    "<div style=\"color:purple\"> \n",
    "<i>Disclaimer:</i> The following dependencies are necessary for the proper execution of the module. In particular, fast-rcnn is only stable for Tensorflow1 (and python 3.6). If you have Tensorflow2 installed on your machine (which I highly recommend!), you will not be able to run the following lines unless you create a virtual environment/docker with the right requirements.\n",
    "</div>\n",
    " \n",
    "```\n",
    "name: keras-frcnn\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.6\n",
    "  - h5py\n",
    "  - numpy\n",
    "  - opencv\n",
    "  - keras-gpu=2.2.4\n",
    "  - cudatoolkit\n",
    "  - tensorflow-gpu=1.14.0\n",
    "  - scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e4101-8596-4321-b29a-5c5685395799",
   "metadata": {},
   "source": [
    "See <a href=\"https://www.docker.com/\">docker.com</a> for some explanations. Assuming you are in the proper environnement, the folowwing command aim to train your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f89bf-d6e9-4768-9c1d-96b15b2a3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python keras-frcnn/train_frcnn.py -o simple -p keras-frcnn/annotate.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436d3b7-0704-4ebf-a4de-2ba6bb331e0e",
   "metadata": {},
   "source": [
    "Training the model will take some time due to the size of the data. If possible, you can use a GPU to speed up the training phase. You can also try reducing the number of epochs as another option. To change the number of epochs, go to the `train_frcnn.py` file in the cloned repository and change the `num_epochs` parameter accordingly.\n",
    "\n",
    "Whenever the model sees an improvement, the weights for that particular epoch will be stored in the same directory as `model_frcnn.hdf5`. These weights will be used when we make predictions about the test set.\n",
    "\n",
    "It can take a long time to train the model and get the weights, depending on your machine setup. \n",
    "You will find on the moodle page of the course weights obtained after training the model for about 500 epochs, which should speed up the estimation. Ensure you save these weights in the cloned repository.\n",
    "\n",
    "<br>\n",
    "\n",
    "Assuming we have successfully trained our model and, indeed, defined the weights, we can now focus on the prediction stage. Keras_frcnn performs the predictions for the new images and saves them in a new folder. We only need to make two changes to the test_frcnn.py file to save the images:\n",
    "1. Remove the comment from the last line of this file: <br>\n",
    "    `cv2.imwrite(‘./results_imgs/{}.png’.format(idx),img)` \n",
    "\n",
    "2. Add comments on the second last and third last line of this file: <br>\n",
    "    `# cv2.imshow(‘img’, img)` <br>\n",
    "    `# cv2.waitKey(0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db401e7-057c-46ff-8dd3-315a63c09f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_frcnn.py -p test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d2904-e915-4d8b-ad5e-2edb464320b9",
   "metadata": {},
   "source": [
    "Finally, the images with the detected objects will be saved in the `results_imgs` folder. Here are some examples of predictions obtained with `Faster R-CNN`:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/51.png\" width=\"300\" style=\"display:inline-block;\"/>&nbsp;\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/5.png\" width=\"300\" style=\"display:inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/89.png\" width=\"300\" style=\"display:inline-block;\"/>&nbsp;\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/11/54.png\" width=\"300\" style=\"display:inline-block;\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037facd-cc68-48c8-9d4f-02fc909e4f7b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Opt2**: Facebook Detectron Software <span style=\"color:purple\">(To Go Further)</span>\n",
    "\n",
    "Another option is to use [Detectron](https://github.com/facebookresearch/detectron2) (2), which  is Facebook AI Research's (FAIR) software system.\n",
    "\n",
    "Detectron requires to work with data encoded in $\\texttt{.json}$ COCO format. However, our data is here encoded under the $\\texttt{.xml}$ VOC format. The first step is then to convert the format of our dataset. To this end, we can use a script such as the one proposed in the repository: https://github.com/yukkyo/voc2coco\n",
    "\n",
    "Moreover, Detectron being powered by FAIR, it requires the use of pytorch (and not keras...). Also, for the sake of brevity, we will not deal with this option (nor with the implementation of the `Mask R-CNN`, which we will \"only\" briefly describe below).\n",
    "\n",
    "<br>\n",
    "\n",
    "Of course, it is always possible to code `faster R-CNN` from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e508acc-2069-4279-af89-df055094fc5b",
   "metadata": {},
   "source": [
    "---\n",
    "## Mask R-CNN <span style=\"color:purple\">(To Go Further)</span>\n",
    "\n",
    "`Mask R-CNN` is state-of-the-art in terms of image segmentation and instance segmentation. It was developed on top of `Faster R-CNN`. While `Faster R-CNN` has 2 outputs for each candidate object, a class label and a bounding-box offset, `Mask R-CNN` is the addition of a third branch that outputs the object mask. The additional mask output is distinct from the class and box outputs, requiring the extraction of a much finer spatial layout of an object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cdfa8-1d0e-46d0-987c-49b6f5111403",
   "metadata": {},
   "source": [
    "---\n",
    "### Image Segmentation\n",
    "\n",
    "The computer vision task _Image Segmentation_ is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). This segmentation is used to locate objects and boundaries (lines, curves, etc.).\n",
    "\n",
    "There are 2 main types of image segmentation that fall under `Mask R-CNN`:\n",
    "1. Semantic Segmentation\n",
    "2. Instance Segmentation\n",
    "\n",
    "<img src=\"https://viso.ai/wp-content/uploads/2021/03/image-segmentation-vs-instance-segmentation.jpg\" width=\"600\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Semantic Segmentation**\n",
    "\n",
    "Semantic segmentation classifies each pixel into a fixed set of categories without differentiating object instances. In other words, semantic segmentation deals with the identification/classification of similar objects as a single class from the pixel level.\n",
    "\n",
    "As shown in the image above, all objects were classified as a single entity (person). Semantic segmentation is otherwise known as background segmentation because it separates the subjects of the image from the background.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Instance Segmentation**\n",
    "\n",
    "Instance Segmentation, or Instance Recognition, deals with the correct detection of all objects in an image while also precisely segmenting each instance. It is, therefore, the combination of object detection, object localization, and object classification. In other words, this type of segmentation goes further to give a clear distinction between each object classified as similar instances.\n",
    "\n",
    "As shown in the example image above, for Instance Segmentation, all objects are persons, but this segmentation process separates each person as a single entity. Semantic segmentation is otherwise known as foreground segmentation because it accentuates the subjects of the image instead of the background.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a3132-6358-47e8-8cee-91c718791c88",
   "metadata": {},
   "source": [
    "---\n",
    "### Facebook Detectron Software <span style=\"color:purple\">(To Go Further)</span>\n",
    "\n",
    "[Mask R-CNN](https://arxiv.org/abs/1703.06870) was introduced by _He et al._ in 2018. It adopts the same two-stage procedure than `Faster R-CNN`, with an identical first stage (the RPN):\n",
    "1. The _first stage_ (RPN) proposes candidate object bounding boxes,\n",
    "2. In the _second stage_, in parallel to predicting the class and box offset, `Mask R-CNN` also outputs a binary mask for each RoI.\n",
    "Formally, during training, they define a multi-task loss on each sampled RoI as $\\mathcal{L}=\\mathcal{L}_{cls}+\\mathcal{L}_{box}+\\mathcal{L}_{mask}$, where $\\mathcal{L}_{cls}$ deals for the classification loss, $\\mathcal{L}_{box}$ the bounding-box loss, and $\\mathcal{L}_{mask}$ the mask one. $\\mathcal{L}_{cls}$ and $\\mathcal{L}_{box}$ are identical as those defined in `Fast R-CNN`. \n",
    "\n",
    "<img src=\"https://viso.ai/wp-content/uploads/2021/03/mask-r-cnn-framework-for-instance-segmentation-1.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902b08e-a15d-4689-82d8-da97f56971b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# PART II: The YOLO (You Only Look Once) Framework\n",
    "---\n",
    "\n",
    "The “You Only Look Once”, or `YOLO`, family of models are a series of end-to-end deep learning models designed for fast object detection, developed by _<a hreh=\"https://pjreddie.com/\">Joseph Redmon</a>, et al._ and first described in the 2015 paper titled “<a href=https://arxiv.org/abs/1506.02640>You Only Look Once: Unified, Real-Time Object Detection</a>”.\n",
    "\n",
    "The Region-Based Convolutional Neural Networks (R-CNNs) family of techniques we saw in Part I primarily use regions to localize the objects within the image. The network does not look at the entire image, only at the parts of the images which have a higher chance of containing an object.\n",
    "\n",
    "The `YOLO` framework on the other hand, deals with object detection in a different way. It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes. The biggest advantage of using YOLO is its superb speed – it’s incredibly fast and can process 45 frames per second. `YOLO` also understands generalized object representation.\n",
    "\n",
    "The approach involves a single deep convolutional neural network (originally a version of `GoogLeNet`, later updated and called `DarkNet` based on `VGG`) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification. The result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.\n",
    "\n",
    "There are variations of the approach but they are all based on the same general idea. Although the accuracy of the models is close but not as good as R-CNNs, they are popular for object detection because of their detection speed, often demonstrated in real-time on video or with camera feed input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bdf21-3d02-430b-b1bb-d030b279ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/MPU2HistivI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0fb04-badb-42d3-922d-ec0c47cec858",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiencor YOLO3 for Keras Project\n",
    "\n",
    "Source code for each version of YOLO is available, as well as pre-trained models.\n",
    "\n",
    "The official <a href=\"https://github.com/pjreddie/darknet\">DarkNet GitHub repository</a> contains the source code for the YOLO versions mentioned in the papers, written in $\\texttt{C}$. The repository provides a step-by-step tutorial on how to use the code for object detection.\n",
    "\n",
    "This is a difficult model to implement from scratch. So we prefer to use a third-party implementation here. There are many third-party implementations designed to use `YOLO` with Keras, but none seem to be standardized and designed to be used as a library. However, the project “<a href=\"https://github.com/experiencor/keras-yolo3\">keras-yolo3: Training and Detecting Objects with YOLO3</a>” or _experiencor_ seems to be mostly used. So we make this choice here. The code of this project has been made available under a permissive MIT open source license.\n",
    "\n",
    "<br>\n",
    "\n",
    "The `keras-yolo3` project provides a lot of capability for using `YOLO3` models, including object detection, transfer learning, and training new models from scratch.\n",
    "\n",
    "Instead of using this program directly, we will reuse elements from this program and develop our own scripts to first prepare and save a Keras `YOLOv3` model, and then load the model to make a prediction for a new photograph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef9bbb-4a66-47f5-b053-d194a35c664f",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "## Create and Save Model\n",
    "\n",
    "The first step is to define a Keras model that has the right number and type of layers to match the downloaded model weights. The model architecture is called a `DarkNet` and was originally loosely based on the `VGG-16` model.\n",
    "\n",
    "<br>\n",
    "\n",
    "The “<a href=\"https://github.com/experiencor/keras-yolo3/blob/master/yolo3_one_file_to_detect_them_all.py\">yolo3_one_file_to_detect_them_all.py</a>” script provides function to create the model for us.\n",
    "\n",
    "* The $\\texttt{_conv_block()}$ function is used to create blocks of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc76de8-ca6f-4c9f-bd2b-46a271c529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    \n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        \n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'], \n",
    "                   conv['kernel'], \n",
    "                   strides=conv['stride'], \n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']), \n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "\n",
    "    return add([skip_connection, x]) if skip else x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89149e6c-fce4-409e-8e6b-8e5b6a53a0b8",
   "metadata": {},
   "source": [
    "* the $\\texttt{make_yolov3_model()}$ function creates the model for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba93c3a-4d84-4e43-ba3f-0edc331b91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "        \n",
    "    skip_36 = x\n",
    "        \n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "        \n",
    "    skip_61 = x\n",
    "        \n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "        \n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014d9e1-2400-48b9-9b8d-b2469ba841fb",
   "metadata": {},
   "source": [
    "Next, we need to download the pre-trained model weights.\n",
    "\n",
    "These were trained using the `DarkNet` code base on the <a href=\"https://cocodataset.org/#home\">COCO</a> dataset. Download the model weights and place them into your current working directory with the filename $\\texttt{yolov3.weights}$. It is a large file and may take a moment to download depending on the speed of your internet connection.\n",
    "\n",
    "<a href=\"https://pjreddie.com/media/files/yolov3.weights\">YOLOv3 Pre-trained Model Weights (yolov3.weights) (237 MB)</a>\n",
    "\n",
    "The model weights are stored in whatever format that was used by `DarkNet`. Rather than trying to decode the file manually, we can use the $\\texttt{WeightReader}$ class provided in the “<a href=\"https://github.com/experiencor/keras-yolo3/blob/master/yolo3_one_file_to_detect_them_all.py\">yolo3_one_file_to_detect_them_all.py</a>” script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605038c-a958-4bfc-9b0e-2116ddf8eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,    = struct.unpack('i', w_f.read(4))\n",
    "            minor,    = struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            \n",
    "            binary = w_f.read()\n",
    "\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "\n",
    "    def load_weights(self, model):\n",
    "        for i in range(106):\n",
    "            try:\n",
    "                conv_layer = model.get_layer('conv_' + str(i))\n",
    "                print(\"loading weights of convolution #\" + str(i))\n",
    "\n",
    "                if i not in [81, 93, 105]:\n",
    "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
    "\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "\n",
    "                    beta  = self.read_bytes(size) # bias\n",
    "                    gamma = self.read_bytes(size) # scale\n",
    "                    mean  = self.read_bytes(size) # mean\n",
    "                    var   = self.read_bytes(size) # variance            \n",
    "\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])  \n",
    "\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    \n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89d5af-82f1-4f73-b9dc-19860b6b27dd",
   "metadata": {},
   "source": [
    "To use the $\\texttt{WeightReader}$, we need to instantiate it with the path to our weight file (in our case $\\texttt{yolov3.weights}$). It will parse the file and load the model weights into memory in a format we can define in our Keras model. \n",
    "We can then call the $\\texttt{load_weights()}$ function of the $\\texttt{WeightReader}$ instance, passing in our defined Keras model to set the weights into the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c7de7-5bb1-4af4-aaaa-532f5c1ce858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = make_yolov3_model()\n",
    "\n",
    "# load the model weights and set the model weights into the model\n",
    "weight_reader = WeightReader('yolov3.weights')\n",
    "weight_reader.load_weights(model)\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61234bc1-e66b-4318-b6ce-f60ed9894d07",
   "metadata": {},
   "source": [
    "At the end of the run, the $\\texttt{model.hdf5}$ file is saved in your current working directory with approximately the same size as the original weight file (237MB), but ready to be loaded and used directly as a Keras model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3314f8-033d-4934-b7b1-d7aa1d5f2e70",
   "metadata": {},
   "source": [
    "---\n",
    "## Make a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95bdd5-0f72-492d-b7c0-d4d9f0f8cfae",
   "metadata": {},
   "source": [
    "We need a new photo for object detection, ideally with objects that we know that the model knows about from the Microsoft's Common Objects in Context (<a href=\"https://cocodataset.org/#home\">COCO</a>) dataset.\n",
    "\n",
    "Here, we will use a photograph of elephants: <a href=\"img/elephants.jpg\">elephants.jpg</a>\n",
    "\n",
    "<img src=\"img/elephants.jpg\"/>\n",
    "\n",
    "But, you can choose whatever picture you want as long as it is an “object” available in COCO: <a href=\"https://cocodataset.org/#explore\">cocodataset.org/#explore</a>.\n",
    "\n",
    "<!-- Download the photograph and place it in your current working directory with the filename ‘zebra.jpg‘. -->\n",
    "\n",
    "The following list of strings contains the known class labels of the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798a276-2555-4e31-9d0e-fdb4891ba4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92906068-f41c-4a78-8fb7-84d060f53dff",
   "metadata": {},
   "source": [
    "Next, we need to load our new photograph and prepare it as suitable input to the model. The model expects inputs to be color images with the square shape of $416\\times416$ pixels.\n",
    "\n",
    "To this end, we use:\n",
    "* the $\\texttt{load_img()}$ Keras function with the $\\texttt{target_size}$ argument to resize the image after loading\n",
    "* and the $\\texttt{img_to_array()}$ function to convert the loaded image object into a `NumPy` array.\n",
    "\n",
    "Also, since we will want to show the original picture again later, we will have to re-scale the future bounding boxes of all detected objects from the square shape to the original shape. Thus, we need to keep the original shape in memory.\n",
    "\n",
    "The following $\\texttt{load_image_pixels()}$ function takes the file name and target size as input and returns the scaled pixel data ready for input to the Keras model, along with the original width and height of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7883c-16ad-473c-a2d6-16c8ec7b844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_pixels(filename, shape):\n",
    "    # load and prepare an image\n",
    "    img = load_img(filename)  # load the image to get its shape\n",
    "    width, height = img.size\n",
    "    img = load_img(filename, target_size=shape)  # load the image with the required size\n",
    "    img = img_to_array(img)  # convert to numpy array\n",
    "    img = img.astype('float32')  # scale pixel values to [0, 1]\n",
    "    img /= 255.0  \n",
    "    img = expand_dims(img, 0)  # add a dimension so that we have one sample\n",
    "    return img, width, height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26418e93-869a-440c-abeb-2c0d63f5c945",
   "metadata": {},
   "source": [
    "We can then call this function to load our photo of elephants, feed the photo into the Keras model and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72458f-01d4-4c66-bf06-57f283fd0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yolov3 model\n",
    "model = load_model('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccd816-00de-409d-8e02-9f317ec8bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the expected input shape for the model\n",
    "input_w, input_h = 416, 416\n",
    "\n",
    "# load and prepare image\n",
    "filename = 'img/elephants.jpg'\n",
    "img, img_w, img_h = load_image_pixels(filename, (input_w, input_h))\n",
    "\n",
    "# make prediction\n",
    "yhat = model.predict(img)\n",
    "\n",
    "print([a.shape for a in yhat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fdb5a-a7d4-4b43-b567-e42cee94e50f",
   "metadata": {},
   "source": [
    "Running the example returns a list of three `NumPy` arrays. These arrays predict both the bounding boxes and class labels but are encoded. They must be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798f338-c7d4-4666-a454-3dd023308d4a",
   "metadata": {},
   "source": [
    "---\n",
    "## Make a Prediction and Interpret Result\n",
    "\n",
    "The output of the model is, in fact, encoded candidate bounding boxes from three different grid sizes, and the boxes are defined the context of anchor boxes, carefully chosen based on an analysis of the size of objects in the <a href=\"https://cocodataset.org/#home\">COCO</a> dataset.\n",
    "\n",
    "Following the official <a href=\"https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\">DarkNet GitHub</a> repository, we will define the anchors as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f79ebe-43a4-4203-a34f-3ae611d7d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b67985-be66-44e0-aa14-c79d9b58083c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The “<a href=\"https://github.com/experiencor/keras-yolo3/blob/master/yolo3_one_file_to_detect_them_all.py\">yolo3_one_file_to_detect_them_all.py</a>” script provides several other functions which will be usefull for the rest of the lab. We propose to import this script now that we are (relatively) familiar with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc02383-7bfd-4e47-a81e-6c76d0956779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_yolo3_one_file_to_detect_them_all as yolo3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61353bb4-9d5c-4b00-9ca1-9f632dfe8a6f",
   "metadata": {},
   "source": [
    "In particular, the script provides the functions:\n",
    "* $\\texttt{decode_netout()}$ that will take each one of the NumPy arrays, one at a time, and decode the candidate bounding boxes and class predictions,\n",
    "* $\\texttt{correct_yolo_boxes()}$ that is able to stretch back the bounding boxes into the shape of the original image. <br> <span style=\"font-size:small;color:gray\">It performs the translation of bounding box coordinates, taking the list of bounding boxes, the original shape of our loaded photograph, and the shape of the input to the network as arguments. The coordinates of the bounding boxes are updated directly.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519d038-7fd4-4189-bc30-4c9fd10f7081",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Any bounding boxes that don’t confidently describe an object (_e.g._ all class probabilities are below a threshold) are ignored. We will use a probability of $60\\%$. The function returns a list of $\\texttt{BoundBox}$ instances that define the corners of each bounding box in the context of the input image shape and class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4de649-db65-4a67-ac33-a933c5246245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_threshold = 0.6  # probability threshold for detected objects\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    boxes += yolo3.decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)  # decode the output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc95b5-5024-4feb-8467-0127408340be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "yolo3.correct_yolo_boxes(boxes, img_h, img_w, input_h, input_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed8bae-4009-418c-9b4b-61c8ecafa4a2",
   "metadata": {},
   "source": [
    "---\n",
    "### IoU: Intersection over Union\n",
    "\n",
    "The model has predicted a lot of candidate bounding boxes, and most of the boxes will be referring to the same objects. The list of bounding boxes can be filtered and those boxes that overlap and refer to the same object can be merged. We can define the amount of overlap as a configuration parameter, in our case, $50\\%$. This filtering of bounding box regions is generally referred to as non-maximal suppression and is a required post-processing step.\n",
    "\n",
    "<br>\n",
    "\n",
    "Intersection over union (IoU) is known to be a good metric for measuring overlap between two bounding boxes or masks. IoU index is literally calculating the “Area of Intersection” of two boxes over the “Area of Union” of the same two boxes. In object detection IoU is calculated between the ground truth bounding box and the predicted bounding boxes to estimate how close our predicted bounding box is to the ground truth.\n",
    "* IoU of 1 means the predicted bounding box perfectly matches the ground truth box.\n",
    "* IoU of 0 means no part of the predicted bounding box overlaps with the ground truth box.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*kK0G-BmCqigHrc1rXs7tYQ.jpeg\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "IoU can be used to remove duplicate detections:\n",
    "1. Pick the bounding box with the largest confidence score and output it as a prediction.\n",
    "2. Compare this IoU of this bounding box with every other predicted bounding box of the same class, and if the IoU is greater than the user-defined IoU threshold, discard it as it’s duplicate detection.\n",
    "3. Remove the outputted predicted bounding box from the list of bounding boxes.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/844/1*9vgYblWTjP0Np7q_PUmlrw.png\" width=\"250\" style=\"display:inline-block;\"/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img src=\"https://miro.medium.com/max/1400/1*6d_D0ySg-kOvfrzIRwHIiA.png\" width=\"450\" style=\"display:inline-block;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The `experiencor` script provides this via the $\\texttt{do_nms()}$ function that takes the list of bounding boxes and a threshold parameter. Rather than purging the overlapping boxes, their predicted probability for their overlapping class is cleared. This allows the boxes to remain and be used if they also detect another object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527e097-d38d-45f2-b48e-27b7e0e2d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress non-maximal boxes\n",
    "yolo3.do_nms(boxes, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3501ec8-10d7-4847-83e6-92a8f273fe47",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "This will leave us with the same number of boxes, but only a few of interest. We can retrieve only those boxes that strongly predict the presence of an object, _i.e._ have a confidence of more than $60\\%$. To do this, we simply enumerate all the boxes and check the class prediction values. We can then search for the corresponding class label for the box and add it to the list. Each box should be considered for each class label, just in case the same box strongly predicts more than one object.\n",
    "\n",
    "We can develop a $\\texttt{get_boxes()}$ function that does this and takes the list of boxes, the known labels, and our classification threshold as arguments and returns parallel lists of boxes, labels, and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d9d0a-291e-4334-a762-45cdf86d0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(boxes, labels, thresh):\n",
    "    # get all of the results above a threshold\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    for box in boxes:\n",
    "        for i in range(len(labels)):\n",
    "            if box.classes[i] > thresh:  # check if the threshold for this label is high enough\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ea88e-53cd-400e-98a7-a713e1706f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374935b-4d08-4128-80be-c76bc10e21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
    "    # draw all results\n",
    "    data = plt.imread(filename)\n",
    "    plt.imshow(data)\n",
    "    ax = plt.gca()  # get the context for drawing boxes\n",
    "\n",
    "    for i in range(len(v_boxes)):\n",
    "        box = v_boxes[i]\n",
    "        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height, fill=False, color='white', linewidth=1.5)\n",
    "        ax.add_patch(rect)  # draw the box\n",
    "        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])  # draw text and score in top left corner\n",
    "        plt.text(x1, y1, label, color='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87149674-e354-4f3e-b6a6-ac1b8789a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727f051-93ea-4ca6-8762-85816647bd27",
   "metadata": {},
   "source": [
    "---\n",
    "## Further predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eb5d0-bda3-40d1-8cb9-e15dabfd59ec",
   "metadata": {},
   "source": [
    "### Zebra\n",
    "\n",
    "<img src=\"img/zebra.jpg\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abcded-9d93-4c62-a2d0-cb707eed9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare image\n",
    "filename = 'img/zebra.jpg'\n",
    "img, img_w, img_h = load_image_pixels(filename, (input_w, input_h))\n",
    "\n",
    "# make prediction\n",
    "yhat = model.predict(img)\n",
    "\n",
    "# create bounding box\n",
    "class_threshold = 0.6\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    boxes += yolo3.decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "    \n",
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "yolo3.correct_yolo_boxes(boxes, img_h, img_w, input_h, input_w)\n",
    "\n",
    "# suppress non-maximal boxes\n",
    "yolo3.do_nms(boxes, 0.5)\n",
    "\n",
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])\n",
    "draw_boxes(filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2ffa3-219e-4f53-b1a6-0c8d4bea982b",
   "metadata": {},
   "source": [
    "---\n",
    "### Savane\n",
    "\n",
    "<img src=\"img/savane.jpg\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e66fe1-6eab-4b1d-9c16-b45968440929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare image\n",
    "filename = 'img/savane.jpg'\n",
    "img, img_w, img_h = load_image_pixels(filename, (input_w, input_h))\n",
    "\n",
    "# make prediction\n",
    "yhat = model.predict(img)\n",
    "\n",
    "# create bounding box\n",
    "class_threshold = 0.6\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    boxes += yolo3.decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "    \n",
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "yolo3.correct_yolo_boxes(boxes, img_h, img_w, input_h, input_w)\n",
    "\n",
    "# suppress non-maximal boxes\n",
    "yolo3.do_nms(boxes, 0.5)\n",
    "\n",
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])\n",
    "draw_boxes(filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90e5d2-6bcc-4f3a-8825-9d294c6df2ce",
   "metadata": {},
   "source": [
    "---\n",
    "### Street\n",
    "\n",
    "<img src=\"img/street.jpg\" width=\"300\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bbdbe-fb61-4a19-9539-6bcebb137c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare image\n",
    "filename = 'img/street.jpg'\n",
    "img, img_w, img_h = load_image_pixels(filename, (input_w, input_h))\n",
    "\n",
    "# make prediction\n",
    "yhat = model.predict(img)\n",
    "\n",
    "# create bounding box\n",
    "class_threshold = 0.6\n",
    "boxes = list()\n",
    "for i in range(len(yhat)):\n",
    "    boxes += yolo3.decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
    "    \n",
    "# correct the sizes of the bounding boxes for the shape of the image\n",
    "yolo3.correct_yolo_boxes(boxes, img_h, img_w, input_h, input_w)\n",
    "\n",
    "# suppress non-maximal boxes\n",
    "yolo3.do_nms(boxes, 0.5)\n",
    "\n",
    "# get the details of the detected objects\n",
    "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
    "\n",
    "# summarize what we found\n",
    "for i in range(len(v_boxes)):\n",
    "    print(v_labels[i], v_scores[i])\n",
    "draw_boxes(filename, v_boxes, v_labels, v_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
